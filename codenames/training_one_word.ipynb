{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ae6e98a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import scipy.spatial.distance\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from game import Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "315d51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_vectors = Game.load_glove_vecs(\"players/glove.6B/glove.6B.300d.txt\")\n",
    "#guesser_vectors = Game.load_w2v(\"players/GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "515d4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 256\n",
    "# Capacity of the replay buffer\n",
    "BUFFER_CAPACITY = 1000 # 10000\n",
    "# Update target net every ... episodes\n",
    "UPDATE_TARGET_EVERY = 20 # 20\n",
    "\n",
    "# Initial value of epsilon\n",
    "EPSILON_START = 1.0\n",
    "# Parameter to decrease epsilon\n",
    "DECREASE_EPSILON = 10000\n",
    "# Minimum value of epislon\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Number of training episodes\n",
    "N_EPISODES = 20000\n",
    "N_TRAINING = 500\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7f42678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.choices(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# create instance of replay buffer\n",
    "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eddde869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f3f4d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "total_wordlist = []\n",
    "with open('players/cm_wordlist.txt') as infile:\n",
    "    for line in infile:\n",
    "        total_wordlist.append(line.rstrip())\n",
    "\n",
    "total_wordlist = np.array(total_wordlist)\n",
    "#wordlist = random.choices(total_wordlist, k=500)\n",
    "mask = np.zeros(len(total_wordlist), dtype=bool)\n",
    "choice = np.random.choice(len(total_wordlist), 500, replace=False)\n",
    "mask[choice] = 1\n",
    "clues = np.extract(mask, total_wordlist)\n",
    "\n",
    "total_board = []\n",
    "with open('game_wordpool.txt') as infile:\n",
    "    for line in infile:\n",
    "        total_board.append(line.rstrip().lower())\n",
    "\n",
    "total_board = np.array(total_board)\n",
    "print(len(total_board))\n",
    "print(len(clues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "543ce88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# create network and target network\n",
    "hidden_size = 128\n",
    "master_size = len(master_vectors[\"word\"])\n",
    "guesser_size = len(guesser_vectors[\"word\"])\n",
    "n_actions = len(clues)\n",
    "\n",
    "q_net_codemaster = Net(master_size, hidden_size, n_actions)\n",
    "if torch.cuda.is_available(): \n",
    "    q_net_codemaster.cuda()\n",
    "target_net_codemaster = Net(master_size, hidden_size, n_actions)\n",
    "if torch.cuda.is_available(): \n",
    "    target_net_codemaster.cuda()\n",
    "\n",
    "# objective and optimizer\n",
    "objective = nn.MSELoss()\n",
    "optimizer = optim.SGD(params=q_net_codemaster.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\"\"\" q_net_guesser = Net(guesser_size, hidden_size, n_actions)\n",
    "if torch.cuda.is_available(): \n",
    "    q_net_guesser.cuda()\n",
    "\n",
    "# objective and optimizer\n",
    "objective_guesser = nn.MSELoss()\n",
    "optimizer_guesser = optim.Adam(params=q_net_guesser.parameters(), lr=LEARNING_RATE) \"\"\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3939915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q(states):\n",
    "    \"\"\"\n",
    "    Compute Q function for a list of states\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        states_v = torch.FloatTensor([states])\n",
    "        states_v = states_v.to(device)\n",
    "        output = q_net_codemaster.forward(states_v).detach().cpu().numpy()  # shape (1, len(states), n_actions)\n",
    "    return output[0, :, :]  # shape (len(states), n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c8b00e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_clue(state, epsilon):\n",
    "    \"\"\"\n",
    "    Return action according to an epsilon-greedy exploration policy\n",
    "    \"\"\"\n",
    "    if np.random.uniform()<epsilon:\n",
    "        #print(\"EXPLORATION\")\n",
    "        index = np.random.randint(0, len(clues))\n",
    "        return index\n",
    "    else:\n",
    "        q=get_q([state])[0]\n",
    "        return q.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "392caad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_word(board, word_vectors, clue):\n",
    "    w2v = []\n",
    "\n",
    "    for word in board:\n",
    "        try:\n",
    "            w2v.append((scipy.spatial.distance.cosine(word_vectors[clue], word_vectors[word.lower()]), word))\n",
    "        except KeyError:\n",
    "            print(\">>> error\")\n",
    "            continue\n",
    "\n",
    "    w2v = list(sorted(w2v))\n",
    "    return w2v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d42dac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn(n_sim=5, test=False, eval_board=None, training=False):\n",
    "    \"\"\"\n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "\n",
    "    if eval_board == None :\n",
    "        eval_board = random.choices(total_board, k=10)\n",
    "\n",
    "    if training :\n",
    "        for sim in range(n_sim):\n",
    "            word_to_guess = eval_board[np.random.randint(0, len(eval_board))]\n",
    "            #print(\">>> To guess : \", word_to_guess)\n",
    "            state=master_vectors[word_to_guess]\n",
    "            action=choose_clue(state, 0.0)\n",
    "            #print(\">>> Clue : \", clues[action])\n",
    "            _, chosen_word = choose_word(eval_board, guesser_vectors, clues[action])\n",
    "            #print(\">>> Chosen : \", chosen_word)\n",
    "            next_state = None\n",
    "            reward = 1 if chosen_word == word_to_guess else 0\n",
    "            episode_rewards[sim]+=reward\n",
    "            if test :\n",
    "                print(\">>> To guess : \", word_to_guess)\n",
    "                print(\">>> Clue : \", clues[action])\n",
    "                print(\">>> Chosen : \", chosen_word)\n",
    "            loss = update(state, action, reward, next_state, True)\n",
    "            print(\">>> Loss : \", loss)\n",
    "            state=next_state\n",
    "    \n",
    "    else :\n",
    "        for sim in range(n_sim):\n",
    "            word_to_guess = eval_board[np.random.randint(0, len(eval_board))]\n",
    "            #print(\">>> To guess : \", word_to_guess)\n",
    "            state=master_vectors[word_to_guess]\n",
    "            action=choose_clue(state, 0.0)\n",
    "            #print(\">>> Clue : \", clues[action])\n",
    "            _, chosen_word = choose_word(eval_board, guesser_vectors, clues[action])\n",
    "            #print(\">>> Chosen : \", chosen_word)\n",
    "            next_state = None\n",
    "            reward = 1 if chosen_word == word_to_guess else 0\n",
    "            episode_rewards[sim]+=reward\n",
    "            state=next_state\n",
    "            if test :\n",
    "                print(\">>> To guess : \", word_to_guess)\n",
    "                print(\">>> Clue : \", clues[action])\n",
    "                print(\">>> Chosen : \", chosen_word)\n",
    "            \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "69007edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    ** TO BE COMPLETED **\n",
    "    \"\"\"\n",
    "\n",
    "    # add data to replay buffer\n",
    "    if done:\n",
    "        next_state = None\n",
    "    replay_buffer.push(state, action, reward, next_state)\n",
    "    \n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        return np.inf\n",
    "    \n",
    "    # get batch\n",
    "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "    \n",
    "    states=[transitions[ii][0] for ii in range(BATCH_SIZE)]\n",
    "    #print(f'STATE : {states}')\n",
    "    actions=[transitions[ii][1] for ii in range(BATCH_SIZE)]\n",
    "    #print(f'ACTIONS : {actions}')\n",
    "    rewards=[transitions[ii][2] for ii in range(BATCH_SIZE)]\n",
    "    #print(f'REWARDS : {rewards}')\n",
    "    next_states=[transitions[ii][3] for ii in range(BATCH_SIZE) if transitions[ii][3] is not None]\n",
    "    #print(f'NEXT_STATES : {next_states}')\n",
    "    mask=[transitions[ii][3] is not None for ii in range(BATCH_SIZE)]\n",
    "    #print(f'MASK : {mask}')\n",
    "    \n",
    "    #convert to tensor\n",
    "    states_torch=torch.FloatTensor(states)\n",
    "    states_torch=states_torch.to(device)\n",
    "    actions_torch=torch.LongTensor(actions).view(-1,1)\n",
    "    actions_torch=actions_torch.to(device)\n",
    "    rewards_torch=torch.FloatTensor(rewards).view(-1,1)\n",
    "    rewards_torch=rewards_torch.to(device)\n",
    "    next_states_torch=torch.FloatTensor(next_states)\n",
    "    next_states_torch=next_states_torch.to(device)\n",
    "    mask_torch=torch.BoolTensor(mask)\n",
    "    mask_torch=mask_torch.to(device)\n",
    "    \n",
    "    #Q(s_i, a_i)\n",
    "    values=q_net_codemaster(states_torch)\n",
    "    values=torch.gather(values, dim=1, index=actions_torch)\n",
    "    \n",
    "    # max_a Q(s_{i+1}, a)\n",
    "    values_next_states=torch.zeros(BATCH_SIZE)\n",
    "    values_next_states=values_next_states.to(device)\n",
    "    values_next_states[mask]=0\n",
    "    values_next_states=values_next_states.view(-1,1)\n",
    "    \n",
    "    #targets y_i\n",
    "    targets=rewards_torch+GAMMA*values_next_states\n",
    "    \n",
    "    #print(f'>>> TARGETS : {targets}')\n",
    "    #print(f'>>> VALUES : {values}')\n",
    "    \n",
    "    loss = objective(values, targets)\n",
    "     \n",
    "    # Optimize the model - UNCOMMENT!\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    loss=loss.cpu()\n",
    "    return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e9a5bd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time =  0\n",
      "on recommence\n",
      "episode = 50 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 100 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 150 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 200 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 250 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 300 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 350 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  349\n",
      "on recommence\n",
      "episode = 400 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 450 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 500 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 550 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 600 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 650 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 700 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 750 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 800 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  450\n",
      "on recommence\n",
      "episode = 850 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 900 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 950 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 1000 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 1050 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 1100 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 1150 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 1200 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 1250 , rewards =  [1. 1. 0. 0. 0.]\n",
      "episode = 1300 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 1350 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 1400 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 1450 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 1500 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 1550 , rewards =  [1. 0. 0. 0. 1.]\n",
      "episode = 1600 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 1650 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 1700 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  500\n",
      "on recommence\n",
      "episode = 1750 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 1800 , rewards =  [0. 1. 0. 1. 0.]\n",
      "episode = 1850 , rewards =  [1. 0. 1. 0. 0.]\n",
      "episode = 1900 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 1950 , rewards =  [0. 1. 1. 1. 0.]\n",
      "episode = 2000 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 2050 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 2100 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 2150 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 2200 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 2250 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 2300 , rewards =  [0. 1. 0. 1. 0.]\n",
      "episode = 2350 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 2400 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 2450 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 2500 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 2550 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 2600 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 2650 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 2700 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 2750 , rewards =  [1. 0. 0. 0. 1.]\n",
      "episode = 2800 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 2850 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 2900 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 2950 , rewards =  [0. 0. 0. 1. 1.]\n",
      "episode = 3000 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 3050 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 3100 , rewards =  [0. 1. 1. 1. 0.]\n",
      "episode = 3150 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 3200 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 3250 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 3300 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 3350 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 3400 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 3450 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 3500 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 3550 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 3600 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 3650 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 3700 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 3750 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 3800 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 3850 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 3900 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 3950 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  500\n",
      "on recommence\n",
      "episode = 4000 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 4050 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 4100 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 4150 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 4200 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 4250 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 4300 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 4350 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 4400 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 4450 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 4500 , rewards =  [1. 1. 0. 0. 1.]\n",
      "episode = 4550 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 4600 , rewards =  [0. 1. 1. 1. 0.]\n",
      "episode = 4650 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 4700 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 4750 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 4800 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 4850 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 4900 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 4950 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 5000 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 5050 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 5100 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 5150 , rewards =  [0. 1. 1. 1. 0.]\n",
      "episode = 5200 , rewards =  [1. 0. 0. 1. 0.]\n",
      "episode = 5250 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 5300 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 5350 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 5400 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 5450 , rewards =  [0. 0. 0. 1. 0.]\n",
      "episode = 5500 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 5550 , rewards =  [1. 1. 0. 0. 1.]\n",
      "episode = 5600 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 5650 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 5700 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 5750 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 5800 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 5850 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 5900 , rewards =  [0. 1. 0. 1. 0.]\n",
      "episode = 5950 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 6000 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 6050 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 6100 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 6150 , rewards =  [0. 0. 0. 1. 0.]\n",
      "episode = 6200 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 6250 , rewards =  [0. 0. 0. 1. 0.]\n",
      "episode = 6300 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 6350 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 6400 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 6450 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 6500 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 6550 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 6600 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 6650 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 6700 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 6750 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 6800 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 6850 , rewards =  [1. 0. 1. 0. 0.]\n",
      "episode = 6900 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 6950 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 7000 , rewards =  [1. 1. 0. 0. 1.]\n",
      "episode = 7050 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 7100 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  350\n",
      "on recommence\n",
      "episode = 7150 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 7200 , rewards =  [1. 1. 0. 0. 0.]\n",
      "episode = 7250 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 7300 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 7350 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 7400 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 7450 , rewards =  [1. 0. 1. 0. 0.]\n",
      "episode = 7500 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 7550 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 7600 , rewards =  [0. 1. 0. 1. 0.]\n",
      "episode = 7650 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 7700 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 7750 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 7800 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 7850 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 7900 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 7950 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 8000 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 8050 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 8100 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 8150 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 8200 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 8250 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 8300 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 8350 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 8400 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 8450 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 8500 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 8550 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 8600 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 8650 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 8700 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 8750 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 8800 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 8850 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 8900 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 8950 , rewards =  [0. 1. 1. 1. 0.]\n",
      "episode = 9000 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 9050 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 9100 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 9150 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 9200 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 9250 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 9300 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 9350 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 9400 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 9450 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 9500 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 9550 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 9600 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 9650 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 9700 , rewards =  [1. 0. 0. 0. 1.]\n",
      "episode = 9750 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 9800 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 9850 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 9900 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 9950 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 10000 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 10050 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 10100 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 10150 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 10200 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 10250 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 10300 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 10350 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 10400 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 10450 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 10500 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  50\n",
      "on recommence\n",
      "episode = 10550 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 10600 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 10650 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 10700 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 10750 , rewards =  [1. 0. 0. 1. 0.]\n",
      "episode = 10800 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 10850 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 10900 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 10950 , rewards =  [1. 1. 0. 0. 1.]\n",
      "episode = 11000 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 11050 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 11100 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 11150 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 11200 , rewards =  [1. 0. 0. 1. 0.]\n",
      "episode = 11250 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 11300 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 11350 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 11400 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 11450 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 11500 , rewards =  [1. 0. 1. 0. 0.]\n",
      "episode = 11550 , rewards =  [1. 0. 1. 0. 0.]\n",
      "episode = 11600 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 11650 , rewards =  [1. 0. 0. 0. 1.]\n",
      "episode = 11700 , rewards =  [1. 0. 0. 0. 1.]\n",
      "episode = 11750 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 11800 , rewards =  [1. 0. 1. 0. 0.]\n",
      "episode = 11850 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 11900 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 11950 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 12000 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 12050 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 12100 , rewards =  [1. 0. 0. 1. 0.]\n",
      "episode = 12150 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 12200 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 12250 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 12300 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 12350 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 12400 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 12450 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 12500 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 12550 , rewards =  [1. 1. 0. 0. 0.]\n",
      "episode = 12600 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 12650 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 12700 , rewards =  [0. 0. 1. 1. 1.]\n",
      "episode = 12750 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 12800 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  900\n",
      "on recommence\n",
      "episode = 12850 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 12900 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 12950 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 13000 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 13050 , rewards =  [1. 0. 0. 1. 0.]\n",
      "episode = 13100 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 13150 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 13200 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 13250 , rewards =  [0. 0. 0. 1. 1.]\n",
      "episode = 13300 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 13350 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 13400 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 13450 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 13500 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 13550 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 13600 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 13650 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  550\n",
      "on recommence\n",
      "episode = 13700 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 13750 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 13800 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 13850 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 13900 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 13950 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 14000 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 14050 , rewards =  [0. 1. 1. 1. 0.]\n",
      "episode = 14100 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 14150 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 14200 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  550\n",
      "on recommence\n",
      "episode = 14250 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 14300 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 14350 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 14400 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 14450 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 14500 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  50\n",
      "on recommence\n",
      "episode = 14550 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 14600 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 14650 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 14700 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 14750 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 14800 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 14850 , rewards =  [1. 1. 0. 0. 0.]\n",
      "episode = 14900 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 14950 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 15000 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 15050 , rewards =  [0. 0. 1. 1. 1.]\n",
      "episode = 15100 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 15150 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 15200 , rewards =  [1. 1. 0. 0. 0.]\n",
      "episode = 15250 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 15300 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 15350 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  400\n",
      "on recommence\n",
      "episode = 15400 , rewards =  [1. 0. 0. 0. 1.]\n",
      "episode = 15450 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 15500 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 15550 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 15600 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 15650 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  200\n",
      "on recommence\n",
      "episode = 15700 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 15750 , rewards =  [1. 1. 0. 0. 1.]\n",
      "episode = 15800 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 15850 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  50\n",
      "on recommence\n",
      "episode = 15900 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 15950 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 16000 , rewards =  [0. 0. 1. 1. 1.]\n",
      "episode = 16050 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 16100 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 16150 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 16200 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 16250 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 16300 , rewards =  [0. 1. 1. 1. 0.]\n",
      "episode = 16350 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 16400 , rewards =  [0. 0. 0. 1. 1.]\n",
      "episode = 16450 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 16500 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  350\n",
      "on recommence\n",
      "episode = 16550 , rewards =  [0. 0. 1. 0. 1.]\n",
      "episode = 16600 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 16650 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 16700 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 16750 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 16800 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  300\n",
      "on recommence\n",
      "episode = 16850 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 16900 , rewards =  [1. 0. 1. 0. 0.]\n",
      "episode = 16950 , rewards =  [1. 1. 0. 0. 0.]\n",
      "episode = 17000 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 17050 , rewards =  [0. 0. 1. 0. 0.]\n",
      "episode = 17100 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 17150 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 17200 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  400\n",
      "on recommence\n",
      "episode = 17250 , rewards =  [1. 0. 0. 0. 0.]\n",
      "episode = 17300 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 17350 , rewards =  [0. 1. 1. 0. 0.]\n",
      "episode = 17400 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 17450 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 17500 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 17550 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 17600 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 17650 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 17700 , rewards =  [1. 0. 0. 1. 0.]\n",
      "episode = 17750 , rewards =  [0. 1. 1. 0. 1.]\n",
      "episode = 17800 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 17850 , rewards =  [1. 0. 1. 1. 0.]\n",
      "episode = 17900 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 17950 , rewards =  [1. 1. 1. 0. 0.]\n",
      "episode = 18000 , rewards =  [0. 1. 0. 1. 1.]\n",
      "episode = 18050 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 18100 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 18150 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 18200 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  650\n",
      "on recommence\n",
      "episode = 18250 , rewards =  [0. 1. 0. 0. 0.]\n",
      "episode = 18300 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  100\n",
      "on recommence\n",
      "episode = 18350 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 18400 , rewards =  [0. 1. 0. 1. 0.]\n",
      "episode = 18450 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  150\n",
      "on recommence\n",
      "episode = 18500 , rewards =  [0. 0. 1. 1. 0.]\n",
      "episode = 18550 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 18600 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 18650 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 18700 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 18750 , rewards =  [0. 0. 1. 1. 1.]\n",
      "episode = 18800 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  350\n",
      "on recommence\n",
      "episode = 18850 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 18900 , rewards =  [1. 0. 0. 1. 1.]\n",
      "episode = 18950 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 19000 , rewards =  [0. 0. 0. 0. 1.]\n",
      "episode = 19050 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 19100 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 19150 , rewards =  [1. 1. 0. 1. 1.]\n",
      "episode = 19200 , rewards =  [0. 1. 0. 1. 0.]\n",
      "episode = 19250 , rewards =  [1. 1. 1. 0. 1.]\n",
      "episode = 19300 , rewards =  [1. 1. 0. 1. 0.]\n",
      "episode = 19350 , rewards =  [1. 1. 0. 0. 1.]\n",
      "episode = 19400 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 19450 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 19500 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  450\n",
      "on recommence\n",
      "episode = 19550 , rewards =  [1. 0. 1. 0. 1.]\n",
      "episode = 19600 , rewards =  [1. 1. 1. 1. 0.]\n",
      "episode = 19650 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 19700 , rewards =  [1. 0. 1. 1. 1.]\n",
      "episode = 19750 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 19800 , rewards =  [0. 0. 0. 1. 1.]\n",
      "episode = 19850 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 19900 , rewards =  [0. 1. 0. 0. 1.]\n",
      "episode = 19950 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 20000 , rewards =  [1. 1. 1. 1. 1.]\n",
      "Time =  250\n",
      "on recommence\n",
      "episode = 20050 , rewards =  [0. 0. 0. 0. 0.]\n",
      "episode = 20100 , rewards =  [0. 1. 1. 1. 1.]\n",
      "episode = 20150 , rewards =  [1. 1. 1. 1. 1.]\n",
      "\n",
      "mean reward after training =  0.15\n",
      ">>> Loss :  0.013828127\n",
      ">>> Loss :  0.01003798\n",
      ">>> Loss :  0.00585945\n",
      ">>> Loss :  0.0070873443\n",
      ">>> Loss :  0.012252116\n",
      ">>> Loss :  0.008394496\n",
      ">>> Loss :  0.008377761\n",
      ">>> Loss :  0.0088949865\n",
      ">>> Loss :  0.012349989\n",
      ">>> Loss :  0.015691219\n",
      ">>> Loss :  0.018240513\n",
      ">>> Loss :  0.007318482\n",
      ">>> Loss :  0.01673242\n",
      ">>> Loss :  0.011723917\n",
      ">>> Loss :  0.013866032\n",
      ">>> Loss :  0.019275501\n",
      ">>> Loss :  0.023234604\n",
      ">>> Loss :  0.00956349\n",
      ">>> Loss :  0.01888247\n",
      ">>> Loss :  0.010165104\n",
      "\n",
      "mean reward with training =  0.5\n"
     ]
    }
   ],
   "source": [
    "EVAL_EVERY = 50\n",
    "REWARD_THRESHOLD = 10\n",
    "\n",
    "def train():\n",
    "    epsilon = EPSILON_START\n",
    "    ep = 0\n",
    "    total_time = 0\n",
    "    time_start = 0\n",
    "    losses = []\n",
    "    while ep < N_EPISODES:\n",
    "        rewards = [0]\n",
    "        board = random.choices(total_board, k=10)\n",
    "        print(\"Time = \", ep - time_start)\n",
    "        time_start = ep\n",
    "        print(\"on recommence\")\n",
    "        while np.mean(rewards) < 1 :\n",
    "            word_to_guess = board[np.random.randint(0, len(board))]\n",
    "            state = master_vectors[word_to_guess]\n",
    "            action = choose_clue(state, epsilon)\n",
    "            chosen_distance, chosen_word = choose_word(board, guesser_vectors, clues[action])\n",
    "            #print(f'Minimal distance = {chosen_distance}')\n",
    "\n",
    "            # take action and update replay buffer and networks\n",
    "            next_state = None\n",
    "            reward = 1 if chosen_word == word_to_guess else 0\n",
    "            done = True\n",
    "            loss = update(state, action, reward, next_state, done)\n",
    "            if loss < np.inf :\n",
    "                losses.append(loss)\n",
    "\n",
    "            # update state\n",
    "            #state = next_state\n",
    "\n",
    "            # end episode if done\n",
    "            if done:\n",
    "                ep += 1\n",
    "                if ( (ep+1)% EVAL_EVERY == 0):\n",
    "                    rewards = eval_dqn(eval_board=board)\n",
    "                    print(\"episode =\", ep+1, \", rewards = \", rewards)\n",
    "                    if np.mean(rewards) >= REWARD_THRESHOLD:\n",
    "                        break\n",
    "\n",
    "                # update target network\n",
    "                if ep % UPDATE_TARGET_EVERY == 0:\n",
    "                    target_net_codemaster.load_state_dict(q_net_codemaster.state_dict())\n",
    "                # decrease epsilon\n",
    "                epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
    "                                np.exp(-1. * ep / DECREASE_EPSILON )    \n",
    "\n",
    "        total_time += 1\n",
    "\n",
    "# Run the training loop\n",
    "train()\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_dqn(20)\n",
    "print(\"\")\n",
    "print(\"mean reward after training = \", np.mean(rewards))\n",
    "rewards = eval_dqn(20, training=True)\n",
    "print(\"\")\n",
    "print(\"mean reward with training = \", np.mean(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
